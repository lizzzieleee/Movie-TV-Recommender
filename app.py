# app.py
import os
from pathlib import Path
from typing import Dict, Any, List, Optional, Tuple

import numpy as np
import pandas as pd
import requests
import streamlit as st
from dotenv import load_dotenv

# ==============================
# Env + constants (v3 API key)
# ==============================
ENV_PATH = Path(__file__).with_name(".env")
load_dotenv(dotenv_path=ENV_PATH)

TMDB_API_KEY = os.getenv("TMDB_API_KEY")
BASE_URL = "https://api.themoviedb.org/3"
IMG_BASE = "https://image.tmdb.org/t/p"  # w92 | w154 | w185 | w342 | w500 | original

MODELS_DIR = Path("models")
MODELS_DIR.mkdir(exist_ok=True)

if not TMDB_API_KEY:
    st.error("‚ö†Ô∏è TMDB_API_KEY not found. Create a .env next to app.py with:\n\nTMDB_API_KEY=your_key_here")
    st.stop()

# ==============================
# Heavy libs for embeddings/index
# ==============================
try:
    import faiss
    from sentence_transformers import SentenceTransformer
except Exception:
    st.error("Missing libraries. Install with:\n\npip install sentence-transformers faiss-cpu")
    st.stop()

@st.cache_resource(show_spinner=False)
def get_embedder():
    return SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

EMBEDDER = get_embedder()

# ==============================
# HTTP helpers (v3 key in params)
# ==============================
def get_tmdb(endpoint: str, params: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
    params = dict(params or {})
    params["api_key"] = TMDB_API_KEY
    url = f"{BASE_URL}{endpoint}"
    r = requests.get(url, params=params, timeout=20)
    r.raise_for_status()
    return r.json()

# --- Helper to build full poster URLs ---
def poster_url(poster_path: str | None, size: str = "w342") -> str | None:
    """Return the full TMDB image URL for a given poster path."""
    return f"{IMG_BASE}/{size}{poster_path}" if poster_path else None

# ==============================
# Rich-detail fetchers (genres/keywords/credits)
# ==============================
@st.cache_data(show_spinner=False, ttl=60 * 60)
def fetch_details_rich(media_type: str, tmdb_id: int) -> Dict[str, Any]:
    """Full details including overview, genres, keywords, cast & crew."""
    details = get_tmdb(f"/{media_type}/{tmdb_id}")
    keywords_data = get_tmdb(f"/{media_type}/{tmdb_id}/keywords")
    credits_data = get_tmdb(f"/{media_type}/{tmdb_id}/credits")

    genres = [g["name"] for g in details.get("genres", [])]
    kw = keywords_data.get("keywords") or keywords_data.get("results") or []
    keywords = [k["name"] for k in kw]

    cast = [c["name"] for c in (credits_data.get("cast") or []) if c.get("name")][:10]
    crew = [c["name"] for c in (credits_data.get("crew") or [])
            if c.get("job") in {"Director", "Writer", "Screenplay"} and c.get("name")][:5]

    return {
        "title": details.get("title") or details.get("name") or "",
        "overview": details.get("overview", "") or "",
        "genres": genres,
        "keywords": keywords,
        "cast": cast,
        "crew": crew,
        "poster_path": details.get("poster_path"),
        "release_date": details.get("release_date") or details.get("first_air_date") or "",
        "runtime": details.get("runtime"),
        "seasons": details.get("number_of_seasons"),
        "episodes": details.get("number_of_episodes"),
        "vote_average": details.get("vote_average"),
        "popularity": details.get("popularity"),
        "homepage": details.get("homepage"),
        "status": details.get("status"),
    }

def make_feature_text_rich(data: Dict[str, Any]) -> str:
    """Overview + 3√óGenres + 2√óKeywords + Cast + Crew."""
    parts = [
        data.get("overview", ""),
        " ".join(data.get("genres", [])) * 3,
        " ".join(data.get("keywords", [])) * 2,
        " ".join(data.get("cast", [])),
        " ".join(data.get("crew", [])),
    ]
    return " ".join([p for p in parts if p]).strip()

# ==============================
# Discover API to build a corpus
# ==============================
def collect_discover(media_type="movie", pages=3, date_gte="2016-01-01", language="en") -> pd.DataFrame:
    rows = []
    date_field = "primary_release_date.gte" if media_type == "movie" else "first_air_date.gte"
    for p in range(1, pages + 1):
        j = get_tmdb(
            f"/discover/{media_type}",
            {
                date_field: date_gte,
                "sort_by": "popularity.desc",
                "include_adult": False,
                "with_original_language": language,
                "page": p,
            },
        )
        for r in j.get("results", []):
            rows.append({
                "tmdb_id": int(r["id"]),
                "media_type": media_type,
                "title": r.get("title") or r.get("name") or "",
                "overview": (r.get("overview") or "").strip(),
                "popularity": float(r.get("popularity") or 0.0),
                "release_date": r.get("release_date") or r.get("first_air_date") or None,
                "poster_path": r.get("poster_path"),
            })
    return pd.DataFrame(rows)

@st.cache_data(show_spinner=True)
def quick_build_corpus(pages_movie=4, pages_tv=3) -> pd.DataFrame:
    """Quick corpus (~150‚Äì250 items) with rich feature text assembled."""
    movies = collect_discover("movie", pages=pages_movie, date_gte="2016-01-01")
    tv = collect_discover("tv", pages=pages_tv, date_gte="2016-01-01")
    pool = pd.concat([movies, tv], ignore_index=True).drop_duplicates(["media_type", "tmdb_id"])

    rows = []
    for _, row in pool.iterrows():
        try:
            rich = fetch_details_rich(row["media_type"], int(row["tmdb_id"]))
            rich["media_type"] = row["media_type"]
            rich["tmdb_id"] = row["tmdb_id"]
            # Prefer details overview if present; fallback to discover
            rich["overview"] = rich.get("overview") or (row.get("overview") or "")
            rich["title"] = row.get("title") or rich.get("title") or ""
            rich["poster_path"] = rich.get("poster_path") or row.get("poster_path")
            rich["release_date"] = row.get("release_date") or rich.get("release_date")
            rich["popularity"] = row.get("popularity", 0.0)
            rich["feature_text"] = make_feature_text_rich(rich)
            rows.append(rich)
        except Exception as e:
            print("[skip]", row.get("title"), e)
            continue

    enriched = pd.DataFrame(rows)
    return enriched

# ==============================
# Index build/load + recommend
# ==============================
def build_index_and_save(enriched: pd.DataFrame) -> faiss.Index:
    texts = enriched["feature_text"].fillna("").tolist()
    X = EMBEDDER.encode(texts, batch_size=256, show_progress_bar=True, normalize_embeddings=True)
    X = np.asarray(X, dtype="float32")
    index = faiss.IndexFlatIP(X.shape[1])  # cosine when normalized
    index.add(X)
    # Save artifacts
    meta_cols = ["media_type", "tmdb_id", "title", "release_date", "popularity", "poster_path"]
    (enriched[meta_cols]).to_parquet(MODELS_DIR / "meta.parquet", index=False)
    faiss.write_index(index, str(MODELS_DIR / "index.faiss"))
    return index

def load_index_and_meta() -> Tuple[pd.DataFrame, Optional[faiss.Index]]:
    meta_path = MODELS_DIR / "meta.parquet"
    index_path = MODELS_DIR / "index.faiss"
    if meta_path.exists() and index_path.exists():
        meta = pd.read_parquet(meta_path)
        index = faiss.read_index(str(index_path))
        return meta, index
    return pd.DataFrame(), None

def feature_text_for_id(media_type: str, tmdb_id: int) -> str:
    rich = fetch_details_rich(media_type, tmdb_id)
    return make_feature_text_rich(rich)

def query_vector_from_selected(liked_items: List[Dict[str, Any]]) -> Optional[np.ndarray]:
    """Builds a query vector directly from selected items (exact IDs)."""
    if not liked_items:
        return None
    texts = []
    for it in liked_items[:5]:  # up to 5
        try:
            txt = feature_text_for_id(it["media_type"], int(it["tmdb_id"]))
            if txt:
                texts.append(txt)
        except Exception:
            pass
    if not texts:
        return None
    V = EMBEDDER.encode(texts, normalize_embeddings=True).astype("float32")
    return V.mean(axis=0, keepdims=True)

def recommend_from_selected(liked_items: List[Dict[str, Any]], meta: pd.DataFrame, index: faiss.Index, k=12, popularity_blend=0.2) -> pd.DataFrame:
    qvec = query_vector_from_selected(liked_items)
    if qvec is None:
        return pd.DataFrame(columns=list(meta.columns) + ["score"])

    m = max(3 * k, 60)
    D, I = index.search(qvec, m)
    recs = meta.iloc[I[0]].copy().reset_index(drop=True)

    liked_set = {(it["media_type"], int(it["tmdb_id"])) for it in liked_items}
    recs = recs[~recs.apply(lambda r: (r["media_type"], int(r["tmdb_id"])) in liked_set, axis=1)]

    if popularity_blend and "popularity" in recs.columns:
        sims = D[0][:len(recs)]
        sims = (sims - sims.min()) / (sims.max() - sims.min() + 1e-8)
        pop = recs["popularity"].to_numpy()
        pop = (pop - pop.min()) / (pop.max() - pop.min() + 1e-8)
        recs["score"] = (1 - popularity_blend) * sims + popularity_blend * pop
        recs = recs.sort_values("score", ascending=False)

    return recs.head(k).reset_index(drop=True)

# ==============================
# UI (Recommender-only) with type-ahead + chips
# ==============================
st.set_page_config(page_title="üé¨ TMDB Recommender", page_icon="üé¨", layout="wide")
st.title("‚ú® Movie & TV Recommender")
st.markdown("""
<style>
.chip {
  display:inline-flex;
  align-items:center;
  background:#f0f2f6;
  border:1px solid #dfe3e8;
  border-radius:999px;
  padding:6px 10px;
  margin:4px 6px 0 0;
  font-size:14px;
  line-height:1.1;
  white-space:nowrap;
  color:#111 !important; /* visible in dark mode */
}
[data-testid="stAppViewContainer"] {
  color-scheme: light dark;
}
</style>
""", unsafe_allow_html=True)

# Session state
if "likes" not in st.session_state:
    st.session_state.likes: List[Dict[str, Any]] = []
if "search_results" not in st.session_state:
    st.session_state.search_results: List[Dict[str, Any]] = []
if "selected_idx" not in st.session_state:
    st.session_state.selected_idx: Optional[int] = None

# Build/Load index
meta, index = load_index_and_meta()
if index is not None and not meta.empty:
    st.success(f"Loaded corpus with {len(meta)} titles.")
else:
    st.warning("No saved index found. Build a quick corpus to enable recommendations.")
    with st.expander("‚öôÔ∏è Build corpus & index (one-time)", expanded=True):
        pages_movie = st.slider("Movie pages (discover)", 2, 8, 4, help="Each page ~20 titles")
        pages_tv = st.slider("TV pages (discover)", 2, 8, 3, help="Each page ~20 titles")
        if st.button("üöÄ Quick-build corpus & index"):
            with st.spinner("Building corpus and index‚Ä¶"):
                enriched = quick_build_corpus(pages_movie=pages_movie, pages_tv=pages_tv)
                if enriched.empty:
                    st.error("Corpus build returned no items.")
                else:
                    index = build_index_and_save(enriched)
                    meta, index = load_index_and_meta()
                    if index is not None and not meta.empty:
                        st.success(f"Built and saved index with {len(meta)} titles.")

st.divider()

# ---------- Type-ahead search + selection ----------
st.subheader("Pick titles you like")

# A form makes Enter/Return submit, and keeps button inline with the input
with st.form("search_form", clear_on_submit=False):
    search_col, button_col = st.columns([5, 1])
    with search_col:
        query = st.text_input(
            "Search for a movie or TV show",
            placeholder="e.g., Succession, Oppenheimer, Dune",
            key="search_query",
            label_visibility="visible",   # or "collapsed" if you want to hide the label
        )
    with button_col:
        submitted = st.form_submit_button("Search", use_container_width=True)

# Run the search only when the form is submitted (button click OR Enter)
if submitted and query.strip():
    try:
        j = get_tmdb("/search/multi", {"query": query.strip(), "include_adult": False, "page": 1})
        results = [
            {
                "title": item.get("title") or item.get("name"),
                "year": (item.get("release_date") or item.get("first_air_date") or "????")[:4],
                "media_type": item.get("media_type"),
                "tmdb_id": int(item.get("id")),
                "poster_path": item.get("poster_path"),
            }
            for item in j.get("results", [])
            if item.get("media_type") in {"movie", "tv"}
        ]
        st.session_state.search_results = results
        st.session_state.selected_idx = None
    except Exception as e:
        st.session_state.search_results = []
        st.session_state.selected_idx = None
        st.warning(f"No results found. ({e})")

# Dropdown fed from last search
options = list(range(len(st.session_state.get("search_results", []))))
labels = [
    f"{r['title']} ({r['year']}) ‚Äî {r['media_type'].upper()}"
    for r in st.session_state.get("search_results", [])
]
selected = st.selectbox(
    "Pick from results",
    options=options if options else [],
    format_func=(lambda i: labels[i] if options else ""),
    index=0 if options else None,
    placeholder="Nothing yet ‚Äî search above",
)

add_cols = st.columns([1, 6])
with add_cols[0]:
    if st.button("Add selected", disabled=not options):
        r = st.session_state.search_results[selected]
        key_set = {(it["media_type"], it["tmdb_id"]) for it in st.session_state.likes}
        if (r["media_type"], r["tmdb_id"]) not in key_set:
            st.session_state.likes.append(r)

# ---------- Chips Row (single renderer; no duplicates) ----------
st.write("")  # tiny spacer
if st.session_state.likes:
    for it in st.session_state.likes.copy():
        c1, c2 = st.columns([0.96, 0.04])   # keep label and ‚úï on the same line
        with c1:
            st.markdown(
                f'<span class="chip">{it["title"]} ({it["media_type"].upper()})</span>',
                unsafe_allow_html=True
            )
        with c2:
            if st.button("‚úï", key=f"rm_{it['media_type']}_{it['tmdb_id']}", help=f"Remove {it['title']}"):
                st.session_state.likes = [
                    x for x in st.session_state.likes
                    if not (x["media_type"] == it["media_type"] and x["tmdb_id"] == it["tmdb_id"])
                ]
                st.rerun()
else:
    st.caption("No titles selected yet.")

# ---------- Recommendation controls ----------
top_k = st.slider("How many recommendations?", 5, 20, 10)
pop_blend = st.slider("Blend with popularity", 0.0, 1.0, 0.2, 0.05, help="0 = pure similarity, 1 = pure popularity")

if st.button("‚ú® Get Recommendations"):
    if not (index is not None and not meta.empty):
        st.error("No index available. Build or load a corpus first.")
    elif not st.session_state.likes:
        st.warning("Add at least one title you like.")
    else:
        with st.spinner("Computing recommendations‚Ä¶"):
            recs = recommend_from_selected(st.session_state.likes, meta, index, k=top_k, popularity_blend=pop_blend)

        if recs.empty:
            st.info("No recommendations found. Try different titles.")
        else:
            st.caption(f"Top {len(recs)} recommendations")
            for _, r in recs.iterrows():
                col1, col2 = st.columns([1, 3], vertical_alignment="top")
                # inside your recs render loop, in the col1 block
                with col1:
                    purl = None
                    # 1) try from meta
                    if "poster_path" in r and pd.notna(r["poster_path"]):
                        purl = poster_url(r["poster_path"], size="w342")
                    # 2) fallback: fetch details (cached) to get poster
                    if not purl:
                        try:
                            d = fetch_details_rich(r["media_type"], int(r["tmdb_id"]))  # cached by st.cache_data
                            if d.get("poster_path"):
                                purl = poster_url(d["poster_path"], size="w342")
                        except Exception:
                            purl = None
                    if purl:
                        st.image(purl, use_container_width=True)
                    else:
                        st.caption("No poster")
                with col2:
                    name = f"**{r['title']}** ‚Äî {r['media_type'].upper()}"
                    date = r.get("release_date") or "‚Äî"
                    st.markdown(f"{name}  \n*Release/Air:* {date}")
                    if "score" in recs.columns:
                        st.write(f"Relevance score: {r['score']:.3f}")

                    with st.expander("üîé View full details"):
                        try:
                            d = fetch_details_rich(r["media_type"], int(r["tmdb_id"]))
                            genres = ", ".join(d.get("genres") or [])
                            cast = ", ".join(d.get("cast") or [])
                            crew = ", ".join(d.get("crew") or [])
                            keywords = ", ".join(d.get("keywords") or [])

                            st.write(d.get("overview") or "‚Äî")
                            st.write(f"**Genres:** {genres or '‚Äî'}")
                            if keywords:
                                st.write(f"**Keywords:** {keywords}")
                            if cast:
                                st.write(f"**Cast (top 10):** {cast}")
                            if crew:
                                st.write(f"**Crew:** {crew}")
                            st.write(f"**Release/Air date:** {d.get('release_date') or '‚Äî'}")
                            if d.get("runtime"):
                                st.write(f"**Runtime:** {d['runtime']} min")
                            if d.get("seasons"):
                                st.write(f"**Seasons:** {d['seasons']}")
                            if d.get("episodes"):
                                st.write(f"**Episodes:** {d['episodes']}")
                            if d.get("vote_average") is not None:
                                st.write(f"**User rating:** {d['vote_average']}")
                            if d.get("popularity") is not None:
                                st.write(f"**Popularity:** {int(d['popularity'])}")
                            if d.get("homepage"):
                                st.write(f"[Homepage]({d['homepage']})")
                            if d.get("status"):
                                st.write(f"**Status:** {d['status']}")
                        except requests.HTTPError as e:
                            st.write(f"Could not fetch details: {e}")
